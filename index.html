<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- ===== Meta tags (å¯ä»¥ä»¥åŽå†ç»†æ”¹) ===== -->
  <meta name="description" content="Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning">
  <meta property="og:title" content="Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end RL"/>
  <meta property="og:description" content="An automatic, collaborative prompting framework where a small LLM generates prompts and a large LLM performs reasoning, trained with end-to-end reinforcement learning."/>
  <meta property="og:url" content="https://QwenQKing.github.io/Prompt-R1/"/>
  <meta property="og:image" content="static/images/1-overview.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Prompt-R1"/>
  <meta name="twitter:description" content="Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning.">
  <meta name="twitter:image" content="static/images/1-overview.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Prompt-R1, prompting, reinforcement learning, large language models, LLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Prompt-R1</title>
  <link rel="icon" type="image/png" href="logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- ===== JS ===== -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

   <!-- ================= Hero + æ ‡é¢˜ ================= -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <!-- ä¸»æ ‡é¢˜ï¼ˆä¸¤è¡Œï¼‰ -->
            <h1 class="title is-1 publication-title" style="line-height:1.25;">
              LexGenius: An Expert-Level Benchmark for Large Language Models in <br>
              Chinese Legal General Intelligence
            </h1>

            <!-- ===== ä½œè€…åˆ—è¡¨ï¼ˆå®Œå…¨æŒ‰ä½ ç»™çš„å›¾æŽ’ç‰ˆï¼‰ ===== -->
            <div class="is-size-4 publication-authors" style="margin-top: 1.2rem;">
              <span class="author-block"><strong>Wenjin Liu</strong><sup>1,2,*</sup></span>
              <span class="author-block"><strong>Haoran Luo</strong><sup>2,*</sup></span>
              <span class="author-block"><strong>Xiang Ji</strong><sup>1</sup></span>
              <span class="author-block"><strong>Xin Feng</strong><sup>1</sup></span>
              <span class="author-block"><strong>Lijuan Zhou</strong><sup>1,â€ </sup></span>
              <br>
              <span class="author-block"><strong>Rui Mao</strong><sup>2</sup></span>
              <span class="author-block"><strong>Jiapu Wang</strong><sup>3</sup></span>
              <span class="author-block"><strong>Shirui Pan</strong><sup>4</sup></span>
              <span class="author-block"><strong>Erik Cambria</strong><sup>2</sup></span>
            </div>

            <!-- ===== å•ä½ä¿¡æ¯ ===== -->
            <div class="is-size-5 publication-authors" style="margin-top: 1rem;">
              <span class="author-block"><sup>1</sup>Hainan University</span><br>
              <span class="author-block"><sup>2</sup>Nanyang Technological University</span><br>
              <span class="author-block"><sup>3</sup>Nanjing University of Science and Technology</span><br>
              <span class="author-block"><sup>4</sup>Griffith University</span><br>
            </div>

            <!-- ===== é‚®ç®± ===== -->
            <div class="is-size-5 publication-authors" style="margin-top: 0.8rem;">
              <span class="author-block">wenjinliu23@outlook.com,&nbsp; haoran.luo@ieee.org</span>
            </div>

            <!-- ===== æŒ‰é’®éƒ¨åˆ†ï¼ˆä½ è¯´å…ˆç•™ç©ºï¼Œä¸åŠ¨ï¼‰ ===== -->
            <div class="column has-text-centered" style="margin-top: 1.4rem;">
              <div class="publication-links">

                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>GitHub</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">ðŸ¤—</span>
                    <span>Dataset</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">ðŸ¤—</span>
                    <span>HF Models</span>
                  </a>
                </span>

              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- =================== å›¾ç‰‡è½®æ’­ï¼ˆç¿»é¡µï¼‰ =================== -->
  <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <!-- Slide 1: Figure 1 -->
        <div class="item">
          <img src="static/images/1-overview.png" alt="Prompt-R1 interaction example"/>
          <h2 class="subtitle has-text-centered">
            An example of Prompt-R1 agent working with a large-scale LLM to answer a question.
            The agent obtains the correct answer step by step through multi-turn prompt interaction.
          </h2>
        </div>

        <!-- Slide 2: Figure 2 -->
        <div class="item">
          <img src="static/images/2-QA.png" alt="Comparison of prompting and optimization methods"/>
          <h2 class="subtitle has-text-centered">
            Comparison of different methods for improving prompt understanding and task adaptability of LLMs:
            humanâ€“LLM interaction, prompt engineering, fine-tuning optimization, and the collaborative automatic prompting
            interaction framework Prompt-R1.
          </h2>
        </div>

        <!-- Slide 3: Figure 3 -->
        <div class="item">
          <img src="static/images/3-Comparison.png" alt="Prompt-R1 framework overview"/>
          <h2 class="subtitle has-text-centered">
            Overview of the Prompt-R1 framework. A small-scale LLM acts as an agent, interacting with a
            large-scale LLM environment through multi-turn prompts. A double-constrained reward is used to optimize the
            prompt agent via end-to-end reinforcement learning.
          </h2>
        </div>

        <!--  Slide 4: Your Table -->
        <div class="item">
          <img src="static/images/4-table1.png" alt="Prompt-R1 comparison table"/>
          <h2 class="subtitle has-text-centered">
            Performance comparison of <strong>Prompt-R1</strong> with state-of-the-art prompting and fine-tuning
            methods on eight public training datasets, including multi-hop reasoning (2WikiMultihopQA, HotpotQA),
            mathematical computation (GSM8K, DAPO Math), standard QA (MusiQue, PopQA), and text generation
            tasks (BookSum, WritingPrompts). <strong>Î”â†‘</strong> denotes the improvement over the GPT-4o-mini
            baseline; higher values indicate stronger performance. Bold numbers mark the best results, and all
            metrics are reported in percentage (%).
          </h2>
        </div>
      
        <!-- Slide 5: OOD Table comparison -->
<div class="item">
  <img src="static/images/5-table2.png" alt="Prompt-R1 out-of-distribution comparison table"/>
  <h2 class="subtitle has-text-centered">
    Performance comparison of <strong>Prompt-R1</strong> with selected baselines on four out-of-distribution
    datasets across multiple tasks, including multi-hop reasoning (TriviaQA), mathematical computation (MathQA),
    standard QA (SQuAD v2), and text generation (XSum). Higher values indicate improved robustness and
    generalization beyond in-distribution training data.
  </h2>
</div>
        <!-- Slide 6: Radar chart comparison -->
<div class="item">
  <img src="static/images/6-radar.png" alt="Radar chart: LLM performance with and without Prompt-R1"/>
  <h2 class="subtitle has-text-centered">
    Comparison of six large language models <strong>with</strong> and <strong>without</strong> the Prompt-R1 agent
    across eight datasets. Metrics include F1 for multi-hop reasoning (2Wiki, Hotpot) and standard QA
    (MusiQue, PopQA), EM for mathematical computation (GSM8K, DAPO), and SSim for text generation
    (BookSum, WritingPrompts). Prompt-R1 brings consistent improvements across all tasks.
  </h2>
</div>
        <!-- Slide 7: Average EM / F1 / SSim with and without Prompt-R1 -->
<div class="item">
  <img src="static/images/7-avg.png"
       alt="Average EM, F1, and SSim scores for six LLMs on OOD datasets, with and without Prompt-R1 agent"/>
  <h2 class="subtitle has-text-centered">
    Comparison of the average <strong>EM</strong>, <strong>F1</strong>, and <strong>SSim</strong> scores for six
    large language models on out-of-distribution datasets, together with the averages obtained when these
    models are equipped with the <strong>Prompt-R1</strong> agent. The hatched bars indicate the performance
    after applying Prompt-R1, showing consistent gains across all metrics.
  </h2>
</div>
        <!-- Slide 8: Ablation Study -->
<div class="item">
  <img src="static/images/8-ablation.png"
       alt="Ablation study of Prompt-R1 under different configurations"/>
  <h2 class="subtitle has-text-centered">
    Ablation analysis of <strong>Prompt-R1</strong> (using GPT-4o-mini as the environment), evaluated on
    both in-distribution and out-of-distribution datasets. The study compares the full Prompt-R1 system
    with three reduced variants: removing the LLM-as-Environment component (<strong>w/o Env.</strong>),
    disabling reinforcement learning (<strong>w/o R.L.</strong>), and removing the Prompt-R1 agent
    (<strong>w/o Agent</strong>). The full model consistently achieves the strongest performance across
    all metrics.
  </h2>
</div>
        <!-- Slide 9: Training process, interaction turns, and metric comparison -->
<div class="item">
  <img src="static/images/9-training.png"
       alt="Training process, interaction turns, and comparisons of EM, F1, and SSim for Prompt-R1"/>
  <h2 class="subtitle has-text-centered">
    (aâ€“b) Training process and interaction turns of the <strong>Prompt-R1</strong> agent
    under different environments (GPT-4o-mini and GPT-OSS-20B).  
    (câ€“e) Comparison of various Prompt-R1 variants with baselines on <strong>average EM</strong>, <strong>average F1</strong>,
    and <strong>average SSim</strong> across in-distribution datasets, showing consistent improvements brought by the
    Prompt-R1 agent.
  </h2>
</div>




      </div>
    </div>
  </div>
</section>

  <!-- ============== è½®æ’­ç»“æŸ ============== -->

  <!-- ============== Abstract ============== -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">

<p>
Legal general intelligence (GI) refers to artificial intelligence (AI) that encompasses legal 
understanding, reasoning, and decision-making, simulating the expertise of legal experts across domains. 
However, existing benchmarks are result-oriented and fail to systematically evaluate the legal 
intelligence of large language models (LLMs), hindering the development of legal GI.
</p>

<p>
To address this, we propose <strong>LexGenius</strong>, an expert-level Chinese legal benchmark for 
evaluating legal GI in LLMs. It follows a <strong>Dimensionâ€“Taskâ€“Ability</strong> framework, covering 
seven dimensions, eleven tasks, and twenty abilities. We use recent legal cases and exam questions to 
create multiple-choice questions, combining manual and LLM reviews to reduce data leakage risks and 
ensure accuracy and reliability through multiple rounds of verification.
</p>

<p>
We evaluate twelve state-of-the-art LLMs on LexGenius and conduct an in-depth analysis. Our findings 
reveal significant disparities across legal intelligence abilities, with even the strongest LLMs still 
lagging behind human legal professionals. We believe LexGenius can serve as a comprehensive benchmark 
for assessing legal intelligence abilities in LLMs and contribute to advancing legal GI development. 
Our project is available at <code>https://github.com/QwenQKing/LexGenius</code>.
</p>

        </div>
      </div>
    </div>
  </div>
</section>

  <!-- ============== Paper PDFï¼ˆå¯æ”¹è·¯å¾„ï¼‰ ============== -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Paper</h2>
        <!-- ä½ ä¹‹åŽå¯ä»¥æŠŠ static/pdfs/prompt-r1.pdf æ¢æˆè‡ªå·±çš„æ–‡ä»¶è·¯å¾„ -->
        <iframe src="static/pdfs/prompt-r1.pdf" width="100%" height="550"></iframe>
      </div>
    </div>
  </section>

  <!-- ============== BibTeX ============== -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{liu2025promptr1collaborativeautomaticprompting,
  title         = {Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning},
  author        = {Wenjin Liu and Haoran Luo and Xueyuan Lin and Haoming Liu and
                   Tiesunlong Shen and Jiapu Wang and Rui Mao and Erik Cambria},
  year          = {2025},
  eprint        = {2511.01016},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2511.01016}
}</code></pre>
    </div>
  </section>

  <!-- ============== Footer ============== -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
                Academic Project Page Template
              </a>
              which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br>
              This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
