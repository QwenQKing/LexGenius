# LexGenius: An Expert-Level Benchmark for Large Language Models in Chinese Legal General Intelligence

<div align="center">

[![arXiv](https://img.shields.io/badge/arXiv-Coming%20Soon-b31b1b.svg)](#)
[![GitHub](https://img.shields.io/badge/GitHub-LexGenius-0A66C2.svg)](https://github.com/QwenQKing/LexGenius)
[![Dataset](https://img.shields.io/badge/Dataset-HuggingFace-orange.svg)](https://huggingface.co/datasets/QwenQKing/LexGenius)
[![HF Models](https://img.shields.io/badge/HF%20Models-HuggingFace-yellow.svg)](https://huggingface.co/QwenQKing/LexGenius)

### **LexGenius**: An Expert-Level Benchmark for Large Language Models in Chinese Legal General Intelligence

[ðŸ“„ Paper](https://arxiv.org/abs/2511.01016) | [ðŸš€ Quick Start](#quick-start-prompt-r1) | [ðŸ’¬ Contact](mailto:wenjinliu23@outlook.com)

</div>

---

## Overview

<div align="center">
  <img src="static/images/compare.png" width="60%"/>
</div>

**LexGenius** addresses a fundamental challenge in applying large language models (LLMs) to the legal domainâ€”the absence of a professional, systematic, and trustworthy evaluation framework for legal intelligence. **LexGenius** is an **expert-level benchmarking suite** designed for Chinese legal scenarios, assessing LLMs across diverse legal tasks and capability dimensions to measure their understanding, reasoning, and normative application skills. Through rigorous dataset construction, realistic legal problem design, and humanâ€“LLM collaborative validation, **LexGenius** significantly enhances the objectivity, discriminability, and reliability of legal intelligence evaluation, offering actionable insights for model development, optimization, and deploymentâ€”without requiring users to build separate assessment systems.


<div align="center">
  <img src="static/images/Framwork.png" width="90%"/>
</div>

By integrating the **three-level structure of seven legal dimensions, eleven tasks, and twenty abilities**, **LexGenius** provides a **structured legal intelligence evaluation framework** that supports systematic capability assessment and cross-model comparative analysis across diverse large-scale LLMs.

## Experimental Results
**Comparison of the 12 SOTA LLMs with human experts on 7 core dimensions of legal intelligence:**
<div align="center">
  <img src="static/images/7 core dimensions.png" width="100%"/>
</div>

**Performance of 12 LLMs and human experts on 11 legal tasks, showing a significant gap between LLMs and humans:**
<div align="center">
  <img src="static/images/11 legal tasks.png" width="100%"/>
</div>

**Average ranking and average score ranking of the 12 SOTA LLMs in the 20 legal intelligence abilities.:**
<div align="center">
  <img src="static/images/20 legal intelligence abilities.png" width="100%"/>
</div>
